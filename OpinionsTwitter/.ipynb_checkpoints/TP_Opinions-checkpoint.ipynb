{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse des opinions sous twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('testdata.manual.2009.06.14.csv',header=None)\n",
    "df.columns = ['polarité','identifiant','date','requête','utilisateur','texte']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prétraitements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupérer le texte associé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segmenter en tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = [t.split(' ') for t in text ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supprimer les urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('http://.*')\n",
    "Tokens=[]\n",
    "for t in tokens:\n",
    "    a = ''\n",
    "    for w in t:        \n",
    "        if not regex.match(w):\n",
    "            a = a + ' ' +  w\n",
    "    Tokens.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "tknz = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "Tokens = [tknz.tokenize(mystring) for mystring in Tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nettoyer les caractères inhérents à la structure d’un tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "regex = re.compile('#|@')\n",
    "Tokens_2=[]\n",
    "for t in Tokens:\n",
    "    a = []\n",
    "    for w in t:        \n",
    "        if not regex.match(w):\n",
    "            a.append(w)\n",
    "    Tokens_2.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corriger les abréviations et les spécificités langagières des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexique = pd.read_csv('Lexiques/SlangLookupTable.txt', header = None,encoding='latin1',sep='\\t')\n",
    "Lex = lexique.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tokens_3=[]\n",
    "for t in Tokens_2:\n",
    "    a = []\n",
    "    for w in t:        \n",
    "        if w not in Lex.index:\n",
    "            a.append(w)\n",
    "        else:\n",
    "            a.append(Lex.loc[w].values[0])\n",
    "    Tokens_3.append(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Etiquetage grammatical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tokens_4 = [nltk.pos_tag(Token) for Token in Tokens_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de mots étiquetés verbes dans le corpus est : 836\n"
     ]
    }
   ],
   "source": [
    "verb=0\n",
    "for T in Tokens_4:\n",
    "    for t in T:\n",
    "        if re.match('VB.',t[1]):\n",
    "            verb+=1\n",
    "print('Le nombre de mots étiquetés verbes dans le corpus est :', verb)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de détection v1 : appel au dictionnaire Sentiwordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiwordnet import SentiWordNetCorpusReader, SentiSynset\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupération des mots correspondant à des adjectifs, noms, adverbes et verbes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Tokens_5=[]\n",
    "for T in Tokens_4:\n",
    "    tt=[]\n",
    "    for t in T:\n",
    "        if re.match('VB.?|NN.?|JJ.?|VB.?|RB.?',t[1]):\n",
    "            tt.append(t[0])\n",
    "    Tokens_5.append(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithme de détection v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "posneg=[]\n",
    "for T in Tokens_5:\n",
    "    a=0\n",
    "    b=0\n",
    "    for t in T:       \n",
    "        syn = wn.synsets(t)\n",
    "        if syn:\n",
    "            senti = swn.senti_synset(syn[0].name())\n",
    "            try:\n",
    "                a+= senti.pos_score\n",
    "                b+= senti.neg_score\n",
    "            except AttributeError:\n",
    "                a+=0\n",
    "                b+=0\n",
    "        else:\n",
    "            a+=0\n",
    "            b+=0\n",
    "    if a<b:\n",
    "        c=0\n",
    "    elif a>b:\n",
    "        c=4\n",
    "    else:\n",
    "        c=2\n",
    "    posneg.append([a,b,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affichage du compte rendu de classification.  le nombre de tweets positifs correctement détectés avec cette version de l’algorithme est à lire dans le Recall_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.39      0.47       177\n",
      "          2       0.47      0.44      0.45       139\n",
      "          4       0.46      0.64      0.54       182\n",
      "\n",
      "avg / total       0.51      0.50      0.49       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print( classification_report(df.iloc[:,0].values,np.array(posneg)[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Algorithme de détection v2 : gestion de la négation et des modifieurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lexique = pd.read_csv('Lexiques/BoosterWordList.txt', header = None,encoding='latin1',sep='\\t')\n",
    "negatif =  pd.read_csv('Lexiques/NegatingWordList.txt', header = None,encoding='latin1',sep='\\t') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tokens_6=[]\n",
    "for T in Tokens_4:\n",
    "    tt=[]\n",
    "    for t in T:\n",
    "        if re.match('VB.?|NN.?|JJ.?|VB.?|RB.?',t[1]):\n",
    "            tt.append(t[0])\n",
    "    Tokens_6.append(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "posneg=[]\n",
    "neginpos = 0\n",
    "for T in Tokens_6:\n",
    "    a=0\n",
    "    b=0\n",
    "    tavt=''\n",
    "    neginpos1 = 0\n",
    "    for t in T: \n",
    "       \n",
    "        syn = wn.synsets(t)\n",
    "        if syn:\n",
    "            senti = swn.senti_synset(syn[0].name())\n",
    "            try:\n",
    "                # Ajout des mots boosts\n",
    "                if tavt in list(lexique.iloc[:,0]):  \n",
    "                    a+= 2*senti.pos_score\n",
    "                    b+= 2*senti.neg_score\n",
    "                # Ajout des négations\n",
    "                elif tavt in list(negatif.iloc[:,0]):\n",
    "                    a+= senti.neg_score\n",
    "                    b+= senti.pos_score\n",
    "                    neginpos1 += 1       \n",
    "                else:\n",
    "                    b+= senti.neg_score\n",
    "                    a+= senti.pos_score\n",
    "            except AttributeError:\n",
    "                a+=0\n",
    "                b+=0\n",
    "        else:\n",
    "            a+=0\n",
    "            b+=0\n",
    "        tavt=t\n",
    "    if a<b:\n",
    "        c=0       \n",
    "    elif a>b:\n",
    "        c=4\n",
    "        neginpos+=neginpos1\n",
    "    else:\n",
    "        c=2       \n",
    "    posneg.append([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.42      0.51       177\n",
      "          2       0.48      0.44      0.46       139\n",
      "          4       0.47      0.64      0.54       182\n",
      "\n",
      "avg / total       0.53      0.51      0.51       498\n",
      "\n",
      "Nombre nombre de termes négatifs contenus dans les tweets positifs : 9\n"
     ]
    }
   ],
   "source": [
    "print( classification_report(df.iloc[:,0].values,np.array(posneg)[:,-1]))\n",
    "print('Nombre nombre de termes négatifs contenus dans les tweets positifs :',neginpos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de détection v3 : gestion des emoticons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emot =  pd.read_csv('Lexiques/EmoticonLookupTable.txt', header = None,encoding='latin1',sep='\\t') \n",
    "emot = emot.set_index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "posneg=[]\n",
    "n_emot=0\n",
    "for T in Tokens_6:\n",
    "    a=0\n",
    "    b=0\n",
    "    tavt=''    \n",
    "    for t in T:       \n",
    "        syn = wn.synsets(t)\n",
    "        if syn:\n",
    "            senti = swn.senti_synset(syn[0].name())\n",
    "            try:\n",
    "                if tavt in list(lexique.iloc[:,0]):  \n",
    "                    a+= 2*senti.pos_score\n",
    "                    b+= 2*senti.neg_score\n",
    "                elif tavt in list(negatif.iloc[:,0]):\n",
    "                    a+= senti.neg_score\n",
    "                    b+= senti.pos_score\n",
    "                else:\n",
    "                    b+= senti.neg_score\n",
    "                    a+= senti.pos_score\n",
    "            except AttributeError:\n",
    "                a+=0\n",
    "                b+=0\n",
    "        # Ajout des émoticones\n",
    "        elif t in list(emot.index):\n",
    "                a+= emot.loc[t].values[0]\n",
    "                b = b\n",
    "                n_emot+=1             \n",
    "        else:\n",
    "            a+=0\n",
    "            b+=0\n",
    "        tavt=t\n",
    "    if a<b:\n",
    "        c=0\n",
    "    elif a>b:\n",
    "        c=4\n",
    "    else:\n",
    "        c=2\n",
    "    posneg.append([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.48      0.56       177\n",
      "          2       0.50      0.44      0.47       139\n",
      "          4       0.50      0.69      0.58       182\n",
      "\n",
      "avg / total       0.56      0.54      0.54       498\n",
      "\n",
      "Le nombre d'emoticones présents dans le corpus est : 57\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df.iloc[:,0].values,np.array(posneg)[:,-1]))\n",
    "print('Le nombre d\\'emoticones présents dans le corpus est :', n_emot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votre version : v4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Améliorations:\n",
    "* Ajout d'un seuil pour la différence du score positif et du score négatif du tweet en deça duquel le tweet est considéré comme neutre. En effet en regardant les erreurs sur neutres, il est apparu que dés qu'il y a une interrogation le tweet est classifié comme neutre.\n",
    "* Tous les mots homonymes sont pris en comptes pour le calcul du score positif et négatif, en les sommant avec des poids w = 1/ordre d'apparition dans les synonimes. Il apparaît en effet que souvent les mots utilisés sur twitter sont utilisés avec le second sens ou plus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "seuil=0.2\n",
    "posneg=[]\n",
    "n_emot=0\n",
    "for T in Tokens_6:\n",
    "    a=0\n",
    "    b=0\n",
    "    tavt=''    \n",
    "    for t in T:       \n",
    "        syn = wn.synsets(t)\n",
    "        if syn:\n",
    "            pos=0\n",
    "            neg=0  \n",
    "            # Calcul des scores avec tous les synonimes (somme pondérées)\n",
    "            for i,sy in enumerate(syn, 1):\n",
    "                if swn.senti_synset(sy.name()) != None and t in sy.name():\n",
    "                    pos +=(1/i)*swn.senti_synset(sy.name()).pos_score\n",
    "                    neg +=(1/i)*swn.senti_synset(sy.name()).neg_score\n",
    "                pos = pos\n",
    "                neg = neg          \n",
    "            if tavt in list(lexique.iloc[:,0]):  \n",
    "                a+= 2*pos\n",
    "                b+= 2*neg\n",
    "            elif tavt in list(negatif.iloc[:,0]):\n",
    "                a+= neg\n",
    "                b+= pos\n",
    "            else:\n",
    "                b+= neg\n",
    "                a+= pos\n",
    "        \n",
    "        elif t in list(emot.index):\n",
    "                a+= 2*emot.loc[t].values[0]\n",
    "                b = b\n",
    "                n_emot+=1             \n",
    "        else:\n",
    "            a+=0\n",
    "            b+=0\n",
    "        tavt=t\n",
    "        \n",
    "    # Calcul de la nouvelle attribution avec un seuil\n",
    "    if abs(a-b) > seuil:      \n",
    "        if a<b:\n",
    "            c=0\n",
    "        else:\n",
    "            c=4\n",
    "    else:\n",
    "        c=2\n",
    "    posneg.append([a,b,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.44      0.55       177\n",
      "          2       0.48      0.62      0.54       139\n",
      "          4       0.55      0.65      0.60       182\n",
      "\n",
      "avg / total       0.60      0.56      0.56       498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df.iloc[:,0].values,np.array(posneg)[:,-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concusion:\n",
    "\n",
    "F1 score \n",
    "    \n",
    "- Algo v1:  0.49\n",
    "- Algo v2: 0.51\n",
    "- Algo v3: 0.54\n",
    "- ALgo v4: 0.56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a amélioré l'algotithme au fur et à mesure des briques ajoutées. Les possibles améliorations sont en enrichissants les différentes bases de données, ou alors en regardant le problème avec des n-uplets de mots."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
